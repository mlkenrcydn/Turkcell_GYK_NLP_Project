{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49958d68-63f8-4807-8d04-98abab000aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2d74a9-777d-40c5-928c-a1e08b784b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melikenurcaydan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unicodedata # Emoji ve özel semboller için\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # küçük harfe çevir\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # noktalama işaretlerini kaldır\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # fazla boşlukları sadeleştir\n",
    "    text = re.sub(r'<.*?>', '', text) # HTML etiketlerini kaldır\n",
    "    text = re.sub(r\"https?://\\S+|www\\S+\", \"\", text) # URL'leri kaldır\n",
    "    text = re.sub(r'#\\S+', '', text) # Hashtag'leri kaldır\n",
    "    text = re.sub(r\"@\\S+\", \"\", text) # metindeki mentionleri kaldır\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17902e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de890d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # veya \"facebook/bart-base\"\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c321fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # article ve highlights birer liste olduğundan her eleman için ayrı ayrı temizle\n",
    "    cleaned_articles = [clean_text(a) for a in batch[\"article\"]]\n",
    "    cleaned_summaries = [clean_text(s) for s in batch[\"highlights\"]]\n",
    "\n",
    "    # her biri için token işlemi uygula\n",
    "    inputs = tokenizer(cleaned_articles, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(cleaned_summaries, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443c02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11490/11490 [00:12<00:00, 939.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6264bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f714dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\") #özetleme görevinde kullanılacak Transformer modeli yüklenir ve ardından Hugging Face'in Trainer API’si için eğitim konfigürasyonlarını tanımlanır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c915b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",                      # model ve log çıktıları buraya kaydedilecek\n",
    "    evaluation_strategy=\"epoch\",                 # her epoch sonunda değerlendirme\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,                          # hızlı prototipleme için 3 epoch\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",   # hangi metrik izlenecek\n",
    "    greater_is_better=False,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de9860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].select(range(2000)),\n",
    "    eval_dataset=tokenized_dataset[\"validation\"].select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # 2 epoch boyunca gelişme yoksa dur\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1556ab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  4%|▍         | 100/2500 [00:56<20:52,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5415, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2500 [01:53<37:25,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5303, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 300/2500 [03:41<32:27,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3437, 'learning_rate': 1.76e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 400/2500 [05:14<30:17,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2996, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [06:47<33:01,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2425, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 500/2500 [07:23<33:01,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9200246930122375, 'eval_runtime': 35.8698, 'eval_samples_per_second': 13.939, 'eval_steps_per_second': 3.485, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 600/2500 [09:06<28:08,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1894, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 700/2500 [10:38<31:03,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.191, 'learning_rate': 1.4400000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 800/2500 [12:15<28:50,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1775, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 900/2500 [13:52<26:16,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1843, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [15:29<23:08,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1292, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1000/2500 [16:04<23:08,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8656452894210815, 'eval_runtime': 35.1798, 'eval_samples_per_second': 14.213, 'eval_steps_per_second': 3.553, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 1100/2500 [17:53<25:35,  1.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.15, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1200/2500 [19:31<22:28,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1129, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1300/2500 [21:13<20:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1398, 'learning_rate': 9.600000000000001e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1400/2500 [22:51<20:16,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1311, 'learning_rate': 8.8e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [24:41<19:39,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0892, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1500/2500 [25:24<19:39,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8523855209350586, 'eval_runtime': 43.8588, 'eval_samples_per_second': 11.4, 'eval_steps_per_second': 2.85, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1600/2500 [27:18<15:25,  1.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1199, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1700/2500 [29:07<14:58,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1151, 'learning_rate': 6.4000000000000006e-06, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1800/2500 [30:56<12:05,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0851, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1900/2500 [32:46<09:51,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1001, 'learning_rate': 4.800000000000001e-06, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [34:29<08:15,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0886, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 2000/2500 [35:07<08:15,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8468968272209167, 'eval_runtime': 37.8398, 'eval_samples_per_second': 13.214, 'eval_steps_per_second': 3.303, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2100/2500 [36:53<06:06,  1.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1123, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2200/2500 [38:35<04:58,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1105, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2300/2500 [40:14<03:04,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0923, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2400/2500 [41:50<01:34,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1118, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [43:10<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.046, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2500/2500 [43:41<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8460965156555176, 'eval_runtime': 30.579, 'eval_samples_per_second': 16.351, 'eval_steps_per_second': 4.088, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 2500/2500 [43:42<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2622.6072, 'train_samples_per_second': 3.813, 'train_steps_per_second': 0.953, 'train_loss': 1.3373460021972656, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.3373460021972656, metrics={'train_runtime': 2622.6072, 'train_samples_per_second': 3.813, 'train_steps_per_second': 0.953, 'train_loss': 1.3373460021972656, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73fa29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c70136a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Haber 1\n",
      "Model Özeti:\n",
      " the 123rd member of the international Criminal Court is a step that gives the court jurisdiction over alleged crimes in palestinians. the ICC opened a preliminary examination into the situation in Palestinian territories.\n",
      "Gerçek Özet:\n",
      " Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "\n",
      "Haber 2\n",
      "Model Özeti:\n",
      " Theia is a friendly white-and-black bully breed mix now named theia. she was found four days after being hit by a car and buried in a field. the dog has been receiving care at the Veterinary Teaching Hospital. she suffered dislocated jaw, leg injuries and a caved-in sinus cavity.\n",
      "Gerçek Özet:\n",
      " Theia, a bully breed mix, was apparently hit by a car, whacked with a hammer and buried in a field .\n",
      "\"She's a true miracle dog and she deserves a good life,\" says Sara Mellado, who is looking for a home for Theia .\n",
      "\n",
      "Haber 3\n",
      "Model Özeti:\n",
      " : Mohammad Javad Zarif is the Iranian foreign minister. he was nominated by former president Mahmoud Ahmadinejad's successor, Hassan Rouhami. he has gone a long way to bring in from the cold and allow it to rejoin the international community.\n",
      "Gerçek Özet:\n",
      " Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister .\n",
      "He once participated in a takeover of the Iranian Consulate in San Francisco .\n",
      "The Iranian foreign minister tweets in English .\n",
      "\n",
      "Haber 4\n",
      "Model Özeti:\n",
      " Five Americans who were monitored for three weeks have been released. one of the five had a heart-related issue on Saturday. none developed the deadly virus.\n",
      "Gerçek Özet:\n",
      " 17 Americans were exposed to the Ebola virus while in Sierra Leone in March .\n",
      "Another person was diagnosed with the disease and taken to hospital in Maryland .\n",
      "National Institutes of Health says the patient is in fair condition after weeks of treatment .\n",
      "\n",
      "Haber 5\n",
      "Model Özeti:\n",
      " \"This is no Duke we want. This is not the Duke we're here to experience,\" a student says. the incident is one of several recent racist events to affect college students.\n",
      "Gerçek Özet:\n",
      " Student is no longer on Duke University campus and will face disciplinary review .\n",
      "School officials identified student during investigation and the person admitted to hanging the noose, Duke says .\n",
      "The noose, made of rope, was discovered on campus about 2 a.m.\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "for i in range(5):\n",
    "    article = tokenized_dataset[\"test\"][i][\"article\"]\n",
    "    reference = tokenized_dataset[\"test\"][i][\"highlights\"]\n",
    "\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512) #Haber metni, tokenizer ile modele uygun formata dönüştürülüyor (tensor olarak).\n",
    "    input_ids = inputs[\"input_ids\"].to(device)  # giriş verisini MPS'e taşı\n",
    "\n",
    "    summary_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nHaber {i+1}\")\n",
    "    print(\"Model Özeti:\\n\", generated_summary)\n",
    "    print(\"Gerçek Özet:\\n\", reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18a3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.3342826888502629\n",
      "ROUGE-2: 0.13014166959372436\n",
      "ROUGE-L: 0.2439524660081373\n"
     ]
    }
   ],
   "source": [
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for i in range(5):\n",
    "    article = tokenized_dataset[\"test\"][i][\"article\"]\n",
    "    reference = tokenized_dataset[\"test\"][i][\"highlights\"]\n",
    "\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    summary_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    generated = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_summaries.append(generated)\n",
    "    reference_summaries.append(reference)\n",
    "\n",
    "# ROUGE hesapla\n",
    "rouge_scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(\"ROUGE-1:\", rouge_scores[\"rouge1\"])\n",
    "print(\"ROUGE-2:\", rouge_scores[\"rouge2\"])\n",
    "print(\"ROUGE-L:\", rouge_scores[\"rougeL\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
